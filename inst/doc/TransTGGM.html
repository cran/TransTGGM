<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Mingyang Ren" />


<title>TransTGGM</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    for (var j = 0; j < rules.length; j++) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") continue;
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') continue;
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">TransTGGM</h1>
<h4 class="author">Mingyang Ren</h4>



<div id="table-of-contents" class="section level1">
<h1>Table of contents</h1>
<ol style="list-style-type: decimal">
<li><a href="#Description">Description</a></li>
<li><a href="#Methodology">Methodology</a></li>
<li><a href="#Quick%20Start">Quick Start</a></li>
</ol>
</div>
<div id="description" class="section level1">
<h1>Description</h1>
<p>Tensor Gaussian graphical models (GGMs), interpreting conditional
independence structures within tensor data, have important applications
in numerous areas. Yet, the available tensor data in one single study is
often limited due to high acquisition costs. Although relevant studies
can provide additional data, it remains an open question how to pool
such heterogeneous data. In this paper, we propose a transfer learning
framework for tensor GGMs, which takes full advantage of informative
auxiliary domains even when non-informative auxiliary domains are
present, benefiting from the carefully designed data-adaptive weights.
Our theoretical analysis shows substantial improvement of estimation
errors and variable selection consistency on the target domain under
much relaxed conditions, by leveraging information from auxiliary
domains.</p>
</div>
<div id="methodology" class="section level1">
<h1>Methodology</h1>
<div id="model-setting" class="section level2">
<h2>Model setting</h2>
<p>Suppose that besides observations <span class="math inline">\(\{
\boldsymbol{\mathcal{X}}_i\}_{i=1}^n\)</span> from the target domain,
observations <span class="math inline">\(\{
\boldsymbol{\mathcal{X}}_i^{(k)}\}_{i=1}^{n_k}\)</span>; <span class="math inline">\(k \in [K]\)</span> from some auxiliary domains are
also available. For example, in the ADHD brain functional network
dataset, <span class="math inline">\(\{
\boldsymbol{\mathcal{X}}_i\}_{i=1}^n\)</span> are the dynamic activation
levels of many brain regions of interests collected from some fMRI scans
at one neuroscience institute, and <span class="math inline">\(\{
\boldsymbol{\mathcal{X}}_i^{(k)}\}_{i=1}^{n_k}\)</span> are collected
from <span class="math inline">\(K=6\)</span> other neuroscience
institutes for better data analysis in the target institute. That is, $
_i$’s are independently generated from <span class="math inline">\(\mathrm{TN}(\boldsymbol{0}; \boldsymbol{\Sigma}_1,
\cdots, \boldsymbol{\Sigma}_M)\)</span> and <span class="math inline">\(\boldsymbol{\mathcal{X}}_i^{(k)}\)</span>’s are
independently generated from $ (; _1^{(k)}, , _M^{(k)})$ with <span class="math inline">\(\Sigma_m \in \mathbb{R}^{p_m \times p_m}\)</span>
and <span class="math inline">\(\boldsymbol{\Sigma}_m^{(k)} \in
\mathbb{R}^{p_m \times p_m}\)</span>. Particularly, we are interested in
estimating the precision matrix <span class="math inline">\(\boldsymbol{\Omega}_m =
(\boldsymbol{\Sigma}_m)^{-1}\)</span> in the target domain for <span class="math inline">\(m\in [M]\)</span> via transfer learning on the
tensor GGMs.</p>
</div>
<div id="proposed-method" class="section level2">
<h2>Proposed method</h2>
<p>Define the divergence matrix as <span class="math inline">\(\boldsymbol{\Delta}_{m}^{(k)} =
\boldsymbol{\Omega}_m \boldsymbol{\Sigma}_m^{(k)} -
\boldsymbol{I}_{p_m}\)</span>, where <span class="math inline">\(\boldsymbol{I}_{p_m}\)</span> is the <span class="math inline">\(p_m\)</span>-dimensional identity matrix. Clearly,
it gets closer to <span class="math inline">\(\boldsymbol{0}\)</span>
when <span class="math inline">\(\boldsymbol{\Sigma}_m^{(k)}\)</span>
gets closer to <span class="math inline">\(\boldsymbol{\Sigma}_m\)</span>, and thus it
provides a natural measure of the similarity between <span class="math inline">\(\boldsymbol{\Sigma}_m^{(k)}\)</span> and <span class="math inline">\(\boldsymbol{\Sigma}_m\)</span>. To leverage
information of all auxiliary domains, we consider the weighted average
of the covariance and divergence matrices as follows, <span class="math display">\[\begin{equation}\nonumber
        \begin{aligned}
            &amp; \boldsymbol{\Sigma}_m^{\mathcal{A}} = \sum_{k=1}^{K}
\alpha_k \boldsymbol{\Sigma}_m^{(k)} \text{ and }
            \boldsymbol{\Delta}_{m} = \sum_{k=1}^{K} \alpha_k
\boldsymbol{\Delta}_m^{(k)}, \text{ with } \sum_{k=1}^{K} \alpha_k = 1,
        \end{aligned}
    \end{equation}\]</span> where the choice of weights <span class="math inline">\(\{\alpha_k\}_{k=1}^{K}\)</span> shall depend on
the contribution of each auxiliary domain and will be discussed in
details in Section <span class="math inline">\(\ref{weights}\)</span>.
Also, it holds true that <span class="math inline">\(\boldsymbol{\Omega}_m
\boldsymbol{\Sigma}_m^{\mathcal{A}} - \boldsymbol{\Delta}_{m} -
\boldsymbol{I}_{p_m} = \boldsymbol{0}\)</span>.</p>
<p>For each mode, a multi-step method can be proposed to realize the
transfer learning of tensor graphical models.</p>
<p>Step 1. Initialization. Estimate <span class="math inline">\(\{
\widehat{\boldsymbol{\Omega}}^{(0)}_m \}_{m=1}^{M}\)</span> based on
target samples ${ <em>i }</em>{i=1}^{n} $, and <span class="math inline">\(\{ \widehat{\boldsymbol{\Omega}}_{m}^{(k)}
\}_{m=1}^{M}\)</span> based on auxiliary samples <span class="math inline">\(\{
\boldsymbol{\mathcal{X}}_i^{(k)}\}_{i=1}^{n_k}\)</span>, for <span class="math inline">\(k \in [K]\)</span>, using the separable estimation
approach ``Tlasso”. Then, define <span class="math display">\[\begin{equation}\nonumber
        \begin{aligned}
            &amp; \widehat{\boldsymbol{\Sigma}}_m^{\mathcal{A}} =
\sum_{k=1}^{K} \alpha_k \widehat{\boldsymbol{\Sigma}}_m^{(k)}, \ \
\text{ where } \widehat{\boldsymbol{\Sigma}}_m^{(k)} =  \frac{p_m}{n_k
p} \sum_{i=1}^{n_k}\widehat{\boldsymbol{V}}_{i,m}^{(k)}
\widehat{\boldsymbol{V}}_{i,m}^{(k) \top}, \\
            &amp; \widehat{\boldsymbol{V}}_{i,m}^{(k)} = [
\boldsymbol{\mathcal{X}}_{i}^{(k)}]_{(m)} \left[  (
\widehat{\boldsymbol{\Omega}}_M^{(k)} )^{1/2} \otimes \cdots \otimes (
\widehat{\boldsymbol{\Omega}}_{m+1}^{(k)} )^{1/2} \otimes
(\widehat{\boldsymbol{\Omega}}_{m-1}^{(k)})^{1/2} \otimes \cdots \otimes
(\widehat{\boldsymbol{\Omega}}_{1}^{(k)})^{1/2} \right].
        \end{aligned}
    \end{equation}\]</span></p>
<p>Step 2. For each <span class="math inline">\(m \in [M]\)</span>,
perform the following two estimation steps separately.</p>
<p>(a). Estimate the divergence matrix of mode-<span class="math inline">\(m\)</span>, <span class="math display">\[\begin{equation}
            \widehat{\boldsymbol{\Delta}}_{m} = \arg \min
\mathcal{Q}_{1} (\boldsymbol{\Delta}_m ),
\end{equation}\]</span> where <span class="math inline">\(\mathcal{Q}_{1} (\boldsymbol{\Delta}_{m} ) =
\frac{1}{2} \operatorname{tr} \{ \boldsymbol{\Delta}_{m}^{\top}
\boldsymbol{\Delta}_{m} \}-\operatorname{tr} \left\{ (
\widehat{\boldsymbol{\Omega}}_m^{(0)}
\widehat{\boldsymbol{\Sigma}}_m^{\mathcal{A}} - \boldsymbol{I}_{p_m} )
^{\top} \boldsymbol{\Delta}_{m} \right\}+ \lambda_{1m} \|
\boldsymbol{\Delta}_{m} \|_1\)</span>.</p>
<pre><code>(b). Estimate the precision matrix of mode-$m$,</code></pre>
<p><span class="math display">\[\begin{equation}
            \widehat{\boldsymbol{\Omega}}_{m} = \arg \min
\mathcal{Q}_{2} ( \boldsymbol{\Omega}_{m} ),
\end{equation}\]</span> where <span class="math inline">\(\mathcal{Q}_{2} ( \boldsymbol{\Omega}_{m}) =
\frac{1}{2} \operatorname{tr} \{ \boldsymbol{\Omega}_{m}^{\top}
\widehat{\boldsymbol{\Sigma}}_m^{\mathcal{A}} \boldsymbol{\Omega}_{m} \}
-\operatorname{tr} \{ ( \widehat{\boldsymbol{\Delta}}_{m}^{\top}+
\boldsymbol{I}_{p_m} ) \boldsymbol{\Omega}_{m} \} + \lambda_{2m} \|
\boldsymbol{\Omega}_{m} \|_{1, \mathrm{off}}\)</span>.</p>
<p>Moreover, the similarity between the target and auxiliary domains may
be weak in some scenarios, so that the learning performance in the
target domain may be deteriorated due to information transfer, which is
so-called ``negative transfer”. One practical solution is to further
perform a model selection step following, which guarantees that transfer
learning is no less effective than using only the target domain. To this
end, the data from the target domain can be randomly split into two
folds <span class="math inline">\(\mathcal{N}\)</span> and <span class="math inline">\(\mathcal{N}^C\)</span>, satisfying <span class="math inline">\(\mathcal{N} \cup \mathcal{N}^C = \{ 1, \cdots, n
\}\)</span> and <span class="math inline">\(\text{card}(\mathcal{N}) =
cn\)</span>, for some constant <span class="math inline">\(0 &lt; c
&lt;1\)</span>. The value of <span class="math inline">\(c\)</span> is
not sensitive, and we set <span class="math inline">\(c=0.6\)</span> in
all numerical experiments. The subjects in <span class="math inline">\(\mathcal{N}\)</span> are used to construct the
initialization of the separable transfer estimation in Step 1. The
selection step is performed based on subjects in <span class="math inline">\(\mathcal{N}^C\)</span>. Specifically, based on
<span class="math inline">\(\{ \widetilde{\boldsymbol{\Omega}}_{m}^{(0)}
\}_{m=1}^{M}\)</span> estimated using subjects in <span class="math inline">\(\mathcal{N}^C\)</span>, for <span class="math inline">\(j = 1, \cdots, p_m\)</span>, define <span class="math inline">\(\widetilde{\boldsymbol{\Sigma}}_m = \frac{p_m}{
(1-c)n p} \sum_{i \in \mathcal{N}^C} \widetilde{\boldsymbol{V}}_{i,m}
\widetilde{\boldsymbol{V}}_{i,m}^{\top}\)</span>, <span class="math inline">\(\widetilde{\boldsymbol{V}}_{i,m} =
[\boldsymbol{\mathcal{X}}_{i}]_{(m)  } \left[ (
\widetilde{\boldsymbol{\Omega}}_M^{(0)} )^{1/2} \otimes \cdots \otimes (
\widetilde{\boldsymbol{\Omega}}_{m+1}^{(0)} )^{1/2} \otimes
(\widetilde{\boldsymbol{\Omega}}_{m-1}^{(0)} )^{1/2} \otimes \cdots
\otimes (\widetilde{\boldsymbol{\Omega}}_{1}^{(0)} )^{1/2}
\right]\)</span>, and <span class="math display">\[\begin{equation}\nonumber
        \begin{aligned}
            &amp; \widehat{w}_{m,j} = \underset{ w \in \{ (0,1)^{\top},
(1,0)^{\top} \} }{\arg\min} \|  \widetilde{\boldsymbol{\Sigma}}_m (
\widehat{\boldsymbol{\Omega}}^{(0)}_{m(j)},
\widehat{\boldsymbol{\Omega}}_{m(j)} ) w -  \boldsymbol{I}_{p_m(j)}
\|_2^2,
        \end{aligned}
    \end{equation}\]</span> where <span class="math inline">\(\widehat{\boldsymbol{\Omega}}^{(0)}_{m(j)}\)</span>,
<span class="math inline">\(\widehat{\boldsymbol{\Omega}}_{m(j)}\)</span>, and
<span class="math inline">\(\boldsymbol{I}_{p_m(j)}\)</span> are the
<span class="math inline">\(j\)</span>-th columns of <span class="math inline">\(\widehat{\boldsymbol{\Omega}}^{(0)}_{m}\)</span>,
<span class="math inline">\(\widehat{\boldsymbol{\Omega}}_{m}\)</span>,
and <span class="math inline">\(\boldsymbol{I}_{p_m}\)</span>,
respectively. Then the final estimate becomes %<span class="math inline">\(\widehat{\boldsymbol{\Omega}}_{m(j)} =
(\widehat{\boldsymbol{\Omega}}^{(0)}_{m(j)},
\widehat{\boldsymbol{\Omega}}_{m(j)}) \widehat{w}_{m,j}\)</span>. <span class="math display">\[\begin{equation}\label{omega_obj}
        \widehat{\boldsymbol{\Omega}}_{m(j)}^{(f)} =
(\widehat{\boldsymbol{\Omega}}^{(0)}_{m(j)},
\widehat{\boldsymbol{\Omega}}_{m(j)}) \widehat{w}_{m(j)}.
    \end{equation}\]</span> The selection step realizes a model
selection between the <span class="math inline">\(\widehat{\boldsymbol{\Omega}}^{(0)}_{m(j)}\)</span>
and <span class="math inline">\(\widehat{\boldsymbol{\Omega}}_{m(j)}\)</span>,
which yields satisfactory theoretical and numerical performance. Note
that <span class="math inline">\(\widehat{\boldsymbol{\Omega}}_{m}^{(f)}\)</span>
is not symmetric in general, and <span class="math inline">\((\widehat{\boldsymbol{\Omega}}_{m}^{(f)} +
[\widehat{\boldsymbol{\Omega}}_{m}^{(f)}]^{\top}) / 2\)</span> can be
used as a symmetric estimate. Furthermore, it can be theoretically
guaranteed that the final estimate is positive definite.</p>
<p>For the weights on auxiliary domains, a natural choice of is to set
<span class="math display">\[\begin{equation}
        \widehat{\boldsymbol{\Sigma}}_m^{\mathcal{A}} = \sum_{k=1}^{K}
\alpha_k \widehat{\boldsymbol{\Sigma}}_m^{(k)}, \ \mbox{with} \ \alpha_k
= n_k / N \ \mbox{and} \  N = \sum_{k=1}^{K} n_k,
    \end{equation}\]</span> following from the fact that the auxiliary
domain with larger sample size shall be more important. Yet, it does not
take into account the similarities between the target and auxiliary
domains. If there are some large non-informative auxiliary domains,
although the final model selection step can guarantee that transfer
learning is no less effective than using the target domain only, it may
also offset the potential improvement benefiting from the informative
auxiliary domains with positive impact.</p>
<p>To address this challenge, we further design some data-adaptive
weights for auxiliary covariance matrices, in which weights are
constructed combining both sample sizes and the estimated differences
between the target and auxiliary domains. Particularly, we set <span class="math display">\[\begin{equation}
\widehat{\boldsymbol{\Sigma}}_m^{\mathcal{A}} = \sum_{k=1}^{K} \alpha_k
\widehat{\boldsymbol{\Sigma}}_m^{(k)}, \text{ with } \alpha_k =
\frac{n_k / \widehat{h}_k}{\sum_{k=1}^{K} (n_k / \widehat{h}_k)},
\end{equation}\]</span> where <span class="math inline">\(\widehat{h}_k
= \max_{m \in [M]} \| \widehat{\boldsymbol{\Delta}}_{m}^{(k)} \|_{1,
\infty}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{\Delta}}_{m}^{(k)} =
\widehat{\boldsymbol{\Omega}}_m^{(0)}
\widehat{\boldsymbol{\Sigma}}_m^{(k)} - \boldsymbol{I}_{p_m}\)</span>.
Clearly, for auxiliary domains with similar sample size, the weight for
the one with smaller difference from the target domain is larger. Here
we note that the type of norm for measuring similarity is not critical,
and the specified <span class="math inline">\(L_1\)</span>-norm is only
for keeping with the form on theoretical analysis and may be replaced by
other norms with slight modification. It is also interesting to note
that even with such data-adaptive weights, the model selection step is
still necessary to safeguard the extreme case where all the auxiliary
domains are non-informative.</p>
</div>
</div>
<div id="quick-start" class="section level1">
<h1>Quick Start</h1>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># example.data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(TransTGGM)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Tlasso)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(example.data)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>t.data <span class="ot">=</span> example.data<span class="sc">$</span>t.data</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>A.data <span class="ot">=</span> example.data<span class="sc">$</span>A.data</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>t.Omega.true.list <span class="ot">=</span> example.data<span class="sc">$</span>t.Omega.true.list</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>normalize <span class="ot">=</span> T</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>K <span class="ot">=</span> <span class="fu">length</span>(A.data)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>p.vec <span class="ot">=</span> <span class="fu">dim</span>(t.data)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>M <span class="ot">=</span> <span class="fu">length</span>(p.vec) <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> p.vec[M<span class="sc">+</span><span class="dv">1</span>]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>p.vec <span class="ot">=</span> p.vec[<span class="dv">1</span><span class="sc">:</span>M]</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>tla.lambda <span class="ot">=</span> <span class="dv">20</span><span class="sc">*</span><span class="fu">sqrt</span>( p.vec<span class="sc">*</span><span class="fu">log</span>(p.vec) <span class="sc">/</span> ( n <span class="sc">*</span> <span class="fu">prod</span>(p.vec) ))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>A.lambda <span class="ot">=</span> <span class="fu">list</span>()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>  A.lambda[[k]] <span class="ot">=</span> <span class="dv">20</span><span class="sc">*</span><span class="fu">sqrt</span>( <span class="fu">log</span>(p.vec) <span class="sc">/</span> ( <span class="fu">dim</span>(A.data[[k]])[M<span class="sc">+</span><span class="dv">1</span>] <span class="sc">*</span> <span class="fu">prod</span>(p.vec) ))</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># the proposed method</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>res.final <span class="ot">=</span> <span class="fu">tensor.GGM.trans</span>(t.data, A.data, A.lambda, <span class="at">normalize =</span> normalize)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Tlasso</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>Tlasso.Omega.list <span class="ot">=</span> <span class="fu">Tlasso.fit</span>(t.data, <span class="at">lambda.vec =</span> tla.lambda, <span class="at">norm.type =</span> <span class="dv">1</span><span class="sc">+</span><span class="fu">as.numeric</span>(normalize))</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># summary</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>i.Omega <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">unlist</span>(<span class="fu">est.analysis</span>(res.final<span class="sc">$</span>Omega.list, t.Omega.true.list))))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>i.Omega.diff <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">unlist</span>(<span class="fu">est.analysis</span>(res.final<span class="sc">$</span>Omega.list.diff, t.Omega.true.list))))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>i.Tlasso <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">t</span>(<span class="fu">unlist</span>(<span class="fu">est.analysis</span>(Tlasso.Omega.list, t.Omega.true.list))))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>i.Omega.diff     <span class="co"># proposed.v</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>i.Omega          <span class="co"># proposed</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>i.Tlasso         <span class="co"># Tlasso</span></span></code></pre></div>
<div id="references" class="section level2">
<h2>References:</h2>
<ul>
<li>Ren, M., Zhen Y., and Wang J. (2022). Transfer learning for tensor
graphical models. (<a href="https://arxiv.org/abs/2211.09391" class="uri">https://arxiv.org/abs/2211.09391</a>)</li>
</ul>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
